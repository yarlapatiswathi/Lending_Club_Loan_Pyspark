{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000)]\n",
    "\n",
    "columns = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "# ------Creating data frame Using createDataFrame() function-----\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+----+------+---------+\n",
      "|  id|  name|     city|\n",
      "+----+------+---------+\n",
      "|1001| Young|Rego Park|\n",
      "|1002| James|    Bronx|\n",
      "|1003|Haseeb|  Astoria|\n",
      "+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "student_dict = {\"id\": [1001, 1002, 1003],\"name\": [ \"Young\", \"James\", \"Haseeb\"],\"city\": [ \"Rego Park\", \"Bronx\", \"Astoria\"]}\n",
    "\n",
    "# --- panda dataframe ----\n",
    "pd_df = pd.DataFrame(student_dict)\n",
    "\n",
    "\n",
    "\n",
    "# ----- SparkSQL dataframe-----\n",
    "sp_df = spark.createDataFrame(pd_df)\n",
    "sp_df.printSchema()\n",
    "sp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "|   _c0|               _c1|      _c2|     _c3|      _c4| _c5|        _c6|       _c7|\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "|   MLS|          Location|    Price|Bedrooms|Bathrooms|Size|Price SQ Ft|    Status|\n",
      "|132842|     Arroyo Grande|795000.00|       3|        3|2371|     335.30|Short Sale|\n",
      "|134364|       Paso Robles|399000.00|       4|        3|2818|     141.59|Short Sale|\n",
      "|135141|       Paso Robles|545000.00|       4|        3|3032|     179.75|Short Sale|\n",
      "|135712|         Morro Bay|909000.00|       4|        4|3540|     256.78|Short Sale|\n",
      "|136282|Santa Maria-Orcutt|109900.00|       3|        1|1249|      87.99|Short Sale|\n",
      "|136431|            Oceano|324900.00|       3|        3|1800|     180.50|Short Sale|\n",
      "|137036|Santa Maria-Orcutt|192900.00|       4|        2|1603|     120.34|Short Sale|\n",
      "|137090|Santa Maria-Orcutt|215000.00|       3|        2|1450|     148.28|Short Sale|\n",
      "|137159|         Morro Bay|999000.00|       4|        3|3360|     297.32|Short Sale|\n",
      "|137570|        Atascadero|319000.00|       3|        2|1323|     241.12|Short Sale|\n",
      "|138053|Santa Maria-Orcutt|350000.00|       3|        2|1750|     200.00|Short Sale|\n",
      "|138730|Santa Maria-Orcutt|249000.00|       3|        2|1400|     177.86|Short Sale|\n",
      "|139291|     Arroyo Grande|299000.00|       2|        2|1257|     237.87|Short Sale|\n",
      "|139427|Santa Maria-Orcutt|235900.00|       3|        2|1400|     168.50|Short Sale|\n",
      "|139461|Santa Maria-Orcutt|348000.00|       3|        2|1600|     217.50|Short Sale|\n",
      "|139661|       Paso Robles|314000.00|       4|        3|1794|     175.03|Short Sale|\n",
      "|139918|        Los Alamos|399000.00|       4|        2|1850|     215.68|Short Sale|\n",
      "|139932|        San Miguel|599000.00|       3|        3|2950|     203.05|Short Sale|\n",
      "|140044|       Paso Robles|299000.00|       3|        2|1719|     173.94|Short Sale|\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Sparkapplicationdemo').getOrCreate()\n",
    "df = spark.read.load(\"Spark_datasets/RealEstate.csv\", format=\"csv\", inferSchema = True)\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               null|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         null|           1|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               null|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         null|           2|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               null|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         null|          10|   PR|           null|      null|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               null|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         null|       61391|   TX|           null|      null|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         null|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         null|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               null|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         null|           4|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         null|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         null|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               null|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         null|       49346|   FL|           null|      null|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         null|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               null|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         null|       49348|   FL|           null|      null|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               null|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         null|          10|   PR|           null|      null|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               null|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         null|           3|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               null|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         null|       54354|   AL|           null|      null|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         null|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         null|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         null|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         null|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"Spark_datasets/zipcode.json\")\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18001c04e80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|      10100|2003-01-06|  2003-01-13| 2003-01-10|Shipped|                null|           363|\n",
      "|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n",
      "|      10102|2003-01-10|  2003-01-18| 2003-01-14|Shipped|                null|           181|\n",
      "|      10103|2003-01-29|  2003-02-07| 2003-02-02|Shipped|                null|           121|\n",
      "|      10104|2003-01-31|  2003-02-09| 2003-02-01|Shipped|                null|           141|\n",
      "|      10105|2003-02-11|  2003-02-21| 2003-02-12|Shipped|                null|           145|\n",
      "|      10106|2003-02-17|  2003-02-24| 2003-02-21|Shipped|                null|           278|\n",
      "|      10107|2003-02-24|  2003-03-03| 2003-02-26|Shipped|Difficult to nego...|           131|\n",
      "|      10108|2003-03-03|  2003-03-12| 2003-03-08|Shipped|                null|           385|\n",
      "|      10109|2003-03-10|  2003-03-19| 2003-03-11|Shipped|Customer requeste...|           486|\n",
      "|      10110|2003-03-18|  2003-03-24| 2003-03-20|Shipped|                null|           187|\n",
      "|      10111|2003-03-25|  2003-03-31| 2003-03-30|Shipped|                null|           129|\n",
      "|      10112|2003-03-24|  2003-04-03| 2003-03-29|Shipped|Customer requeste...|           144|\n",
      "|      10113|2003-03-26|  2003-04-02| 2003-03-27|Shipped|                null|           124|\n",
      "|      10114|2003-04-01|  2003-04-07| 2003-04-02|Shipped|                null|           172|\n",
      "|      10115|2003-04-04|  2003-04-12| 2003-04-07|Shipped|                null|           424|\n",
      "|      10116|2003-04-11|  2003-04-19| 2003-04-13|Shipped|                null|           381|\n",
      "|      10117|2003-04-16|  2003-04-24| 2003-04-17|Shipped|                null|           148|\n",
      "|      10118|2003-04-21|  2003-04-29| 2003-04-26|Shipped|Customer has work...|           216|\n",
      "|      10119|2003-04-28|  2003-05-05| 2003-05-02|Shipped|                null|           382|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"jdbc\").options(driver=\"com.mysql.cj.jdbc.Driver\",\\\n",
    "                                     user=\"root\",\\\n",
    "                                     password=\"admin\",\\\n",
    "                                     url=\"jdbc:mysql://localhost:3306/classicmodels\",\\\n",
    "                                     dbtable=\"classicmodels.orders\"\n",
    "                                    ).load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.types import *\n",
    "schema = StructType([ \\\n",
    "    StructField(\"orderNumber\",IntegerType(),False), \\\n",
    "    StructField(\"orderDate\",DateType(),True), \\\n",
    "    StructField(\"requiredDate\",DateType(),True), \\\n",
    "    StructField(\"shippedDate\", DateType(), True), \\\n",
    "    StructField(\"status\", StringType(), True), \\\n",
    "    StructField(\"comments\", StringType(), True), \\\n",
    "    StructField(\"customerNumber\", IntegerType(), True)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderNumber: integer (nullable = true)\n",
      " |-- orderDate: date (nullable = true)\n",
      " |-- requiredDate: date (nullable = true)\n",
      " |-- shippedDate: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- customerNumber: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+----+------+---------+\n",
      "|  id|  name|     city|\n",
      "+----+------+---------+\n",
      "|1001| Young|Rego Park|\n",
      "|1002| James|    Bronx|\n",
      "|1003|Haseeb|  Astoria|\n",
      "+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_dict = {\"id\": [1001, 1002, 1003],\"name\": [ \"Young\", \"James\", \"Haseeb\"],\"city\": [ \"Rego Park\", \"Bronx\", \"Astoria\"]}\n",
    "schema = StructType([ \\\n",
    "    StructField(\"id\",IntegerType(),False), \\\n",
    "    StructField(\"name\",StringType(),True), \\\n",
    "    StructField(\"city\",StringType(),True)\n",
    "  ])\n",
    "\n",
    "\n",
    "# ----- SparkSQL dataframe-----\n",
    "sp_df = spark.createDataFrame(pd_df,schema=schema)\n",
    "sp_df.printSchema()\n",
    "sp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---+------+------+-------+\n",
      "|rollno|     name|age|height|weight|address|\n",
      "+------+---------+---+------+------+-------+\n",
      "|   001|     john| 23|  5.79|    67|     NY|\n",
      "|   002|    James| 18|  3.79|    34|     NY|\n",
      "|   003|     Eric| 17|  2.79|    17|     NJ|\n",
      "|   004|Shahparan| 19|  3.69|    28|     NJ|\n",
      "|   005|     Flex| 37|  5.59|    54| Dallas|\n",
      "+------+---------+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "#and import struct types and data types\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
    "spark_app = SparkSession.builder.appName('sparkdemo').getOrCreate()\n",
    "\n",
    "# ------create student data with 5 rows and 6 attributes------\n",
    "students =[['001','john',23,5.79,67,'NY'], ['002','James',18,3.79,34,'NY'], ['003','Eric',17,2.79,17,'NJ'],\n",
    "               ['004','Shahparan',19,3.69,28,'NJ'],['005','Flex',37,5.59,54,'Dallas']]\n",
    "\n",
    "#----------define the StructType and StructFields-------\n",
    "#for the below column names\n",
    "schema=StructType([\n",
    "    StructField(\"rollno\",StringType(),True),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"age\",IntegerType(),True),\n",
    "    StructField(\"height\", FloatType(), True),\n",
    "    StructField(\"weight\", IntegerType(), True),\n",
    "    StructField(\"address\", StringType(), True)  ])\n",
    " \n",
    "#-----create the dataframe and add schema to the dataframe---\n",
    "df = spark_app.createDataFrame(students, schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rollno: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height: float (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(rollno,StringType,true),StructField(name,StringType,true),StructField(age,IntegerType,true),StructField(height,FloatType,true),StructField(weight,IntegerType,true),StructField(address,StringType,true)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(rollno,StringType,true),\n",
       " StructField(name,StringType,true),\n",
       " StructField(age,IntegerType,true),\n",
       " StructField(height,FloatType,true),\n",
       " StructField(weight,IntegerType,true),\n",
       " StructField(address,StringType,true)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col,array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayStructureData = [\n",
    "        ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    "        ]\n",
    "        \n",
    "#-----define the StructType and StructFields----\n",
    "arrayStructureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('languages', ArrayType(StringType()), True),\n",
    "         StructField('state', StringType(), True),\n",
    "         StructField('gender', StringType(), True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data = arrayStructureData, schema = arrayStructureSchema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state == \"OH\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"jdbc\").options(driver=\"com.mysql.cj.jdbc.Driver\",\\\n",
    "                                     user=\"root\",\\\n",
    "                                     password=\"admin\",\\\n",
    "                                     url=\"jdbc:mysql://localhost:3306/classicmodels\",\\\n",
    "                                     dbtable=\"classicmodels.orders\").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderNumber: integer (nullable = true)\n",
      " |-- orderDate: date (nullable = true)\n",
      " |-- requiredDate: date (nullable = true)\n",
      " |-- shippedDate: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- customerNumber: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|      10100|2003-01-06|  2003-01-13| 2003-01-10|Shipped|                null|           363|\n",
      "|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n",
      "|      10102|2003-01-10|  2003-01-18| 2003-01-14|Shipped|                null|           181|\n",
      "|      10103|2003-01-29|  2003-02-07| 2003-02-02|Shipped|                null|           121|\n",
      "|      10104|2003-01-31|  2003-02-09| 2003-02-01|Shipped|                null|           141|\n",
      "|      10105|2003-02-11|  2003-02-21| 2003-02-12|Shipped|                null|           145|\n",
      "|      10106|2003-02-17|  2003-02-24| 2003-02-21|Shipped|                null|           278|\n",
      "|      10107|2003-02-24|  2003-03-03| 2003-02-26|Shipped|Difficult to nego...|           131|\n",
      "|      10108|2003-03-03|  2003-03-12| 2003-03-08|Shipped|                null|           385|\n",
      "|      10109|2003-03-10|  2003-03-19| 2003-03-11|Shipped|Customer requeste...|           486|\n",
      "|      10110|2003-03-18|  2003-03-24| 2003-03-20|Shipped|                null|           187|\n",
      "|      10111|2003-03-25|  2003-03-31| 2003-03-30|Shipped|                null|           129|\n",
      "|      10112|2003-03-24|  2003-04-03| 2003-03-29|Shipped|Customer requeste...|           144|\n",
      "|      10113|2003-03-26|  2003-04-02| 2003-03-27|Shipped|                null|           124|\n",
      "|      10114|2003-04-01|  2003-04-07| 2003-04-02|Shipped|                null|           172|\n",
      "|      10115|2003-04-04|  2003-04-12| 2003-04-07|Shipped|                null|           424|\n",
      "|      10116|2003-04-11|  2003-04-19| 2003-04-13|Shipped|                null|           381|\n",
      "|      10117|2003-04-16|  2003-04-24| 2003-04-17|Shipped|                null|           148|\n",
      "|      10118|2003-04-21|  2003-04-29| 2003-04-26|Shipped|Customer has work...|           216|\n",
      "|      10119|2003-04-28|  2003-05-05| 2003-05-02|Shipped|                null|           382|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------+--------+---------+----+-----------+----------+\n",
      "|   MLS|          Location|   Price|Bedrooms|Bathrooms|Size|Price SQ Ft|    Status|\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+\n",
      "|132842|     Arroyo Grande|795000.0|       3|        3|2371|      335.3|Short Sale|\n",
      "|134364|       Paso Robles|399000.0|       4|        3|2818|     141.59|Short Sale|\n",
      "|135141|       Paso Robles|545000.0|       4|        3|3032|     179.75|Short Sale|\n",
      "|135712|         Morro Bay|909000.0|       4|        4|3540|     256.78|Short Sale|\n",
      "|136282|Santa Maria-Orcutt|109900.0|       3|        1|1249|      87.99|Short Sale|\n",
      "|136431|            Oceano|324900.0|       3|        3|1800|      180.5|Short Sale|\n",
      "|137036|Santa Maria-Orcutt|192900.0|       4|        2|1603|     120.34|Short Sale|\n",
      "|137090|Santa Maria-Orcutt|215000.0|       3|        2|1450|     148.28|Short Sale|\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+\n",
      "only showing top 8 rows\n",
      "\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+-----------------+\n",
      "|   MLS|          Location|   Price|Bedrooms|Bathrooms|Size|Price SQ Ft|    Status|PriceAfterService|\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+-----------------+\n",
      "|132842|     Arroyo Grande|795000.0|       3|        3|2371|      335.3|Short Sale|         796000.0|\n",
      "|134364|       Paso Robles|399000.0|       4|        3|2818|     141.59|Short Sale|         400000.0|\n",
      "|135141|       Paso Robles|545000.0|       4|        3|3032|     179.75|Short Sale|         546000.0|\n",
      "|135712|         Morro Bay|909000.0|       4|        4|3540|     256.78|Short Sale|         910000.0|\n",
      "|136282|Santa Maria-Orcutt|109900.0|       3|        1|1249|      87.99|Short Sale|         110900.0|\n",
      "|136431|            Oceano|324900.0|       3|        3|1800|      180.5|Short Sale|         325900.0|\n",
      "|137036|Santa Maria-Orcutt|192900.0|       4|        2|1603|     120.34|Short Sale|         193900.0|\n",
      "|137090|Santa Maria-Orcutt|215000.0|       3|        2|1450|     148.28|Short Sale|         216000.0|\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+-----------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Sparkapplicationdemo').getOrCreate()\n",
    "df = spark.read.load(\"Spark_datasets/RealEstate.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "df.show(8)\n",
    "new_df= df.withColumn('PriceAfterService', df.Price + 1000)\n",
    "# Checking the Updated DataFrame\n",
    "new_df.show(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MLS', 'Location', 'Price', 'Bedrooms', 'Bathrooms', 'Size', 'Price SQ Ft', 'Status', 'PriceAfterService']\n",
      "781\n",
      "779\n",
      "== Physical Plan ==\n",
      "*(1) Project [MLS#693, Location#694, Price#695, Bedrooms#696, Bathrooms#697, Size#698, Price SQ Ft#699, Status#700, (Price#695 + 1000.0) AS PriceAfterService#751]\n",
      "+- FileScan csv [MLS#693,Location#694,Price#695,Bedrooms#696,Bathrooms#697,Size#698,Price SQ Ft#699,Status#700] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/shash/pyspark_practice/Spark_datasets/RealEstate.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<MLS:int,Location:string,Price:double,Bedrooms:int,Bathrooms:int,Size:int,Price SQ Ft:doubl...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns columns of dataframe\n",
    "print(new_df.columns)\n",
    "# Counts number of rows in dataframe\n",
    "print(new_df.count())\n",
    "\n",
    "# Counts number of distinct rows in dataframe\n",
    "print(new_df[['MLS']].distinct().count())\n",
    "\n",
    "# Prints plans including physical and logical\n",
    "new_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          HW Ratio|\n",
      "+------------------+\n",
      "|21.834285714285713|\n",
      "|20.263157894736842|\n",
      "|             24.48|\n",
      "|35.666666666666664|\n",
      "| 33.36666666666667|\n",
      "|28.416666666666668|\n",
      "|              null|\n",
      "| 36.27777777777778|\n",
      "| 29.42222222222222|\n",
      "|             26.34|\n",
      "|             27.89|\n",
      "|             29.14|\n",
      "|  28.5979381443299|\n",
      "|             29.45|\n",
      "|             29.01|\n",
      "| 34.27777777777778|\n",
      "|26.927272727272726|\n",
      "|             32.88|\n",
      "|             24.48|\n",
      "|33.018181818181816|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "cardf = spark.read.load(\"Spark_datasets/cars.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "\n",
    "cardf.withColumn('HW Ratio', cardf['Weight'] / cardf['Horsepower']).select('HW Ratio').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|quantity|   city|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|AMC Ambassador Br...|13.0|        8|       360.0|       175|  3821|        11.0|   73|    US|      25|NewYork|\n",
      "|  AMC Ambassador DPL|15.0|        8|       390.0|       190|  3850|         8.5|   70|    US|       2|     NJ|\n",
      "|  AMC Ambassador SST|17.0|        8|       304.0|       150|  3672|        11.5|   72|    US|       4| DALLAS|\n",
      "|         AMC Concord|19.4|        6|       232.0|        90|  3210|        17.2|   78|    US|      52|  TEXAS|\n",
      "|         AMC Concord|24.3|        4|       151.0|        90|  3003|        20.1|   80|    US|      42|     OH|\n",
      "|     AMC Concord d/l|18.1|        6|       258.0|       120|  3410|        15.1|   78|    US|       4|NewYork|\n",
      "|      AMC Concord DL|23.0|        4|       151.0|         0|  3035|        20.5|   82|    US|      45|     NJ|\n",
      "|    AMC Concord DL 6|20.2|        6|       232.0|        90|  3265|        18.2|   79|    US|     328| DALLAS|\n",
      "|         AMC Gremlin|21.0|        6|       199.0|        90|  2648|        15.0|   70|    US|      68|  TEXAS|\n",
      "|         AMC Gremlin|19.0|        6|       232.0|       100|  2634|        13.0|   71|    US|      78|     OH|\n",
      "|         AMC Gremlin|18.0|        6|       232.0|       100|  2789|        15.0|   73|    US|     152|NewYork|\n",
      "|         AMC Gremlin|20.0|        6|       232.0|       100|  2914|        16.0|   75|    US|     214|     NJ|\n",
      "|          AMC Hornet|18.0|        6|       199.0|        97|  2774|        15.5|   70|    US|      60| DALLAS|\n",
      "|          AMC Hornet|18.0|        6|       232.0|       100|  2945|        16.0|   73|    US|     144|  TEXAS|\n",
      "|          AMC Hornet|19.0|        6|       232.0|       100|  2901|        16.0|   74|    US|     172|     OH|\n",
      "|          AMC Hornet|22.5|        6|       232.0|        90|  3085|        17.6|   76|    US|      28|NewYork|\n",
      "|AMC Hornet Sporta...|18.0|        6|       258.0|       110|  2962|        13.5|   71|    US|      90|     NJ|\n",
      "|         AMC Matador|18.0|        6|       232.0|       100|  3288|        15.5|   71|    US|      82| DALLAS|\n",
      "|         AMC Matador|14.0|        8|       304.0|       150|  3672|        11.5|   73|    US|     131|  TEXAS|\n",
      "|         AMC Matador|16.0|        6|       258.0|       110|  3632|        18.0|   74|    US|     179|NewYork|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|      Location|        avg(Price)|\n",
      "+--------------+------------------+\n",
      "|   Pismo Beach| 772374.5833333334|\n",
      "|     King City|          131190.0|\n",
      "|    New Cuyama|           40900.0|\n",
      "|        Nipomo| 454166.6666666667|\n",
      "|        Oceano|          392640.0|\n",
      "|        Nipomo| 430629.4117647059|\n",
      "|     Templeton| 705890.9090909091|\n",
      "| Arroyo Grande|1013958.3333333334|\n",
      "|   Bakersfield|           91500.0|\n",
      "|     Guadalupe|          117250.0|\n",
      "+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Sparkapplicationdemo').getOrCreate()\n",
    "# Reading the Dataset\n",
    "df = spark.read.load(\"Spark_datasets/RealEstate.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "\n",
    "df.select(\"Location\", \"Price\").groupby(\"Location\").avg().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employeeNumber: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- extension: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- officeCode: integer (nullable = true)\n",
      " |-- reportsTo: integer (nullable = true)\n",
      " |-- jobTitle: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- employeeNumber: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- extension: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- officeCode: integer (nullable = true)\n",
      " |-- reportsTo: integer (nullable = true)\n",
      " |-- jobTitle: string (nullable = true)\n",
      "\n",
      "+--------------+---------+---------+--------------------+----------+--------------------+-----+\n",
      "|employeeNumber|firstName| lastName|            jobTitle|officeCode|        addressLine1|state|\n",
      "+--------------+---------+---------+--------------------+----------+--------------------+-----+\n",
      "|          1002|    Diane|   Murphy|           President|         1|   100 Market Street|   CA|\n",
      "|          1056|     Mary|Patterson|            VP Sales|         1|   100 Market Street|   CA|\n",
      "|          1076|     Jeff| Firrelli|        VP Marketing|         1|   100 Market Street|   CA|\n",
      "|          1088|  William|Patterson|Sales Manager (APAC)|         6|5-11 Wentworth Av...| NULL|\n",
      "|          1102|   Gerard|   Bondur| Sale Manager (EMEA)|         4|     43 Rue Jouffroy|   NY|\n",
      "|          1143|  Anthony|      Bow|  Sales Manager (NA)|         1|   100 Market Street|   CA|\n",
      "|          1165|   Leslie| Jennings|           Sales Rep|         1|   100 Market Street|   CA|\n",
      "|          1166|   Leslie| Thompson|           Sales Rep|         1|   100 Market Street|   CA|\n",
      "|          1188|    Julie| Firrelli|           Sales Rep|         2|    1550 Court Place|   MA|\n",
      "|          1216|    Steve|Patterson|           Sales Rep|         2|    1550 Court Place|   MA|\n",
      "+--------------+---------+---------+--------------------+----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf = spark.read.load(\"Spark_datasets/employee.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "empdf.printSchema()\n",
    "\n",
    "officedf = spark.read.load(\"Spark_datasets/officecode.csv\", format=\"csv\", header = True  ,inferSchema = True)\n",
    "empdf.printSchema()\n",
    "\n",
    "empdf.createTempView(\"employee_table\")\n",
    "officedf.createTempView(\"office_table\")\n",
    "\n",
    "spark.sql(\"SELECT employee_table.employeeNumber, employee_table.firstName,employee_table.lastName,jobTitle, employee_table.officeCode, \\\n",
    "office_table.addressLine1, office_table.state  FROM employee_table \\\n",
    "join office_table ON office_table.officeCode = employee_table.officeCode\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
